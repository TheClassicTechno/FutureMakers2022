{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Julia Huang of Day_9_Introduction_to_Loss_Functions_SV.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheClassicTechno/FutureMakers2022/blob/main/day9/Julia_Huang_of_Day_9_Introduction_to_Loss_Functions_SV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "861ncVuLPeyF"
      },
      "source": [
        "![image_2021-10-30_133041.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 9 Objectives: \n",
        "* To introduce you to loss functions. \n"
      ],
      "metadata": {
        "id": "5rokz_Xr0kDG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCtJpzBrYWqr"
      },
      "source": [
        "# Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4fAWIkUYUyR"
      },
      "source": [
        "Loss functions define what a good prediction is and isn’t. Choosing the right loss function dictates how well your estimator (machine learning model) will be. The criteria by which an estimator is scrutinized is its performance - how accurate the model's decisions are. This calls for a way to measure how far a particular iteration of the model is from the actual values. This is where loss functions come into play.\n",
        "\n",
        "Loss functions measure how far an estimated value is from its true value. A loss function maps decisions to their associated costs. Loss functions are not fixed, they change depending on the task in hand and the goal to be met.\n",
        "\n",
        "Worth to note we can speak of different kind of loss functions: **regression loss** functions and **classification loss** functions.\n",
        "\n",
        "Regression loss function describes the difference between the values that a model is predicting and the actual values of the labels. So the loss function has a meaning on a labeled data when we compare the prediction to the label at a single point of time. This loss function is often called the error function or the error formula. Typical error functions we use for regression models are L1 and L2, Huber loss, Quantile loss, log cosh loss.\n",
        "\n",
        "**Note**: L1 loss is also know as Mean Absolute Error. L2 Loss is also know as Mean Square Error or Quadratic loss.\n",
        "\n",
        "Loss functions for classification represent the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to). To name a few: log loss, focal loss, exponential loss, hinge loss, relative entropy loss and other.\n",
        "\n",
        "*Note*: While more commonly used in regression, the square loss function can be re-written and utilized for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I1Y9BQxq72l"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Hpicvr6XJ0"
      },
      "source": [
        "# Regression Losses\n",
        "\n",
        "Remember, in regression, the output would be a real value. We need some loss functions which compares two real values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMoFFw2VUR7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92b4c8e4-b683-4946-938b-187497ea8105"
      },
      "source": [
        "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()\n",
        "\n",
        "# get per-feature statistics (mean, standard deviation) from the training set to normalize by\n",
        "train_mean = np.mean(train_features, axis=0)\n",
        "train_std = np.std(train_features, axis=0)\n",
        "train_features = (train_features - train_mean) / train_std"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n",
            "65536/57026 [==================================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAdSFpdB6aDH"
      },
      "source": [
        "## Mean Squared Error [MSE]\n",
        "\n",
        "As the name suggests, Mean square error is measured as the average of squared difference between predictions and actual observations. It’s only concerned with the average magnitude of error irrespective of their direction. \n",
        "\n",
        "However, due to squaring, predictions which are far away from actual values are penalized heavily in comparison to less deviated predictions. Plus MSE has nice mathematical properties which makes it easier to calculate gradients.\n",
        "\n",
        "Let's assume there are $n$ data samples, for $i^{th}$ sample; the actual output is $y_i$ and $\\hat{y}_i$ is the estimated output from the regression model. \n",
        "\n",
        "We first square the difference between the original and estimated output with $(y_i - \\hat{y}_i)^2$. Then we take sum of the squared difference for all the samples. And finally divide it by the total count of samples, which is $n$. \n",
        "\n",
        "$$MSE = \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5OO5ZfoYtCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "852c7ed1-ffa5-4089-a90c-f621fef26392"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(10, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mse',\n",
        "              metrics=['mse'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 3s 18ms/step - loss: 569.1574 - mse: 569.1574 - val_loss: 470.9711 - val_mse: 470.9711\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 541.0369 - mse: 541.0369 - val_loss: 441.7174 - val_mse: 441.7174\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 502.7634 - mse: 502.7634 - val_loss: 401.7328 - val_mse: 401.7328\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 448.2224 - mse: 448.2224 - val_loss: 346.2331 - val_mse: 346.2331\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 373.8715 - mse: 373.8715 - val_loss: 271.9263 - val_mse: 271.9263\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 277.0286 - mse: 277.0286 - val_loss: 182.4608 - val_mse: 182.4608\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 175.5591 - mse: 175.5591 - val_loss: 97.3551 - val_mse: 97.3551\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 96.2150 - mse: 96.2150 - val_loss: 49.1174 - val_mse: 49.1174\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 68.0026 - mse: 68.0026 - val_loss: 36.1279 - val_mse: 36.1279\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 58.1537 - mse: 58.1537 - val_loss: 29.4483 - val_mse: 29.4483\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 50.3170 - mse: 50.3170 - val_loss: 24.2991 - val_mse: 24.2991\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 44.0893 - mse: 44.0893 - val_loss: 20.7463 - val_mse: 20.7463\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 39.1660 - mse: 39.1660 - val_loss: 18.2507 - val_mse: 18.2507\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 35.6838 - mse: 35.6838 - val_loss: 16.0901 - val_mse: 16.0901\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 32.4640 - mse: 32.4640 - val_loss: 14.7406 - val_mse: 14.7406\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 29.9097 - mse: 29.9097 - val_loss: 13.8559 - val_mse: 13.8559\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 27.7940 - mse: 27.7940 - val_loss: 13.1612 - val_mse: 13.1612\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 26.1340 - mse: 26.1340 - val_loss: 12.6500 - val_mse: 12.6500\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 24.7181 - mse: 24.7181 - val_loss: 12.4205 - val_mse: 12.4205\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 23.6279 - mse: 23.6279 - val_loss: 12.0982 - val_mse: 12.0982\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 22.6205 - mse: 22.6205 - val_loss: 11.9127 - val_mse: 11.9127\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 21.8337 - mse: 21.8337 - val_loss: 11.9324 - val_mse: 11.9324\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 21.1342 - mse: 21.1342 - val_loss: 11.9159 - val_mse: 11.9159\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 20.6198 - mse: 20.6198 - val_loss: 11.8243 - val_mse: 11.8243\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 20.0487 - mse: 20.0487 - val_loss: 11.8399 - val_mse: 11.8399\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 19.5949 - mse: 19.5949 - val_loss: 11.4817 - val_mse: 11.4817\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 19.3388 - mse: 19.3388 - val_loss: 11.6744 - val_mse: 11.6744\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18.9215 - mse: 18.9215 - val_loss: 11.5778 - val_mse: 11.5778\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18.6327 - mse: 18.6327 - val_loss: 11.4098 - val_mse: 11.4098\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18.2878 - mse: 18.2878 - val_loss: 11.2255 - val_mse: 11.2255\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18.0288 - mse: 18.0288 - val_loss: 11.1821 - val_mse: 11.1821\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 17.7623 - mse: 17.7623 - val_loss: 11.6867 - val_mse: 11.6867\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 17.5992 - mse: 17.5992 - val_loss: 10.9813 - val_mse: 10.9813\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 17.1943 - mse: 17.1943 - val_loss: 11.0268 - val_mse: 11.0268\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16.9578 - mse: 16.9578 - val_loss: 10.9286 - val_mse: 10.9286\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 16.7310 - mse: 16.7310 - val_loss: 10.8694 - val_mse: 10.8694\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 16.5286 - mse: 16.5286 - val_loss: 10.6475 - val_mse: 10.6475\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16.4034 - mse: 16.4034 - val_loss: 10.4111 - val_mse: 10.4111\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16.1332 - mse: 16.1332 - val_loss: 10.3607 - val_mse: 10.3607\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 15.9178 - mse: 15.9178 - val_loss: 10.4158 - val_mse: 10.4158\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.7338 - mse: 15.7338 - val_loss: 10.1933 - val_mse: 10.1933\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 15.5459 - mse: 15.5459 - val_loss: 9.8640 - val_mse: 9.8640\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.3867 - mse: 15.3867 - val_loss: 10.0436 - val_mse: 10.0436\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.2089 - mse: 15.2089 - val_loss: 9.9033 - val_mse: 9.9033\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.0669 - mse: 15.0669 - val_loss: 9.6288 - val_mse: 9.6288\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 14.9096 - mse: 14.9096 - val_loss: 9.7551 - val_mse: 9.7551\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.7590 - mse: 14.7590 - val_loss: 9.5485 - val_mse: 9.5485\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.6201 - mse: 14.6201 - val_loss: 9.5140 - val_mse: 9.5140\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.4165 - mse: 14.4165 - val_loss: 9.4982 - val_mse: 9.4982\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.2430 - mse: 14.2430 - val_loss: 9.4673 - val_mse: 9.4673\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 14.1764 - mse: 14.1764 - val_loss: 9.2407 - val_mse: 9.2407\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.0455 - mse: 14.0455 - val_loss: 9.3614 - val_mse: 9.3614\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 13.8777 - mse: 13.8777 - val_loss: 9.2285 - val_mse: 9.2285\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.8049 - mse: 13.8049 - val_loss: 9.2109 - val_mse: 9.2109\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.5934 - mse: 13.5934 - val_loss: 8.8749 - val_mse: 8.8749\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.5209 - mse: 13.5209 - val_loss: 9.0539 - val_mse: 9.0539\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.4141 - mse: 13.4141 - val_loss: 8.9630 - val_mse: 8.9630\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 13.2447 - mse: 13.2447 - val_loss: 8.9718 - val_mse: 8.9718\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.0380 - mse: 13.0380 - val_loss: 9.0957 - val_mse: 9.0957\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.0338 - mse: 13.0338 - val_loss: 8.9962 - val_mse: 8.9962\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.8991 - mse: 12.8991 - val_loss: 9.0346 - val_mse: 9.0346\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12.8452 - mse: 12.8452 - val_loss: 8.9441 - val_mse: 8.9441\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.6676 - mse: 12.6676 - val_loss: 8.9580 - val_mse: 8.9580\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12.6639 - mse: 12.6639 - val_loss: 8.8836 - val_mse: 8.8836\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.6259 - mse: 12.6259 - val_loss: 8.9093 - val_mse: 8.9093\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.3880 - mse: 12.3880 - val_loss: 9.0008 - val_mse: 9.0008\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12.4614 - mse: 12.4614 - val_loss: 9.0107 - val_mse: 9.0107\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12.2632 - mse: 12.2632 - val_loss: 8.7923 - val_mse: 8.7923\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12.1614 - mse: 12.1614 - val_loss: 8.8830 - val_mse: 8.8830\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 12.2151 - mse: 12.2151 - val_loss: 8.8724 - val_mse: 8.8724\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.0416 - mse: 12.0416 - val_loss: 8.7677 - val_mse: 8.7677\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.0154 - mse: 12.0154 - val_loss: 8.6524 - val_mse: 8.6524\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.8948 - mse: 11.8948 - val_loss: 8.6605 - val_mse: 8.6605\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.8140 - mse: 11.8140 - val_loss: 8.6576 - val_mse: 8.6576\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.7872 - mse: 11.7872 - val_loss: 8.7010 - val_mse: 8.7010\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.7812 - mse: 11.7812 - val_loss: 8.4576 - val_mse: 8.4576\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.7182 - mse: 11.7182 - val_loss: 8.7065 - val_mse: 8.7065\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.7234 - mse: 11.7234 - val_loss: 8.5830 - val_mse: 8.5830\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.5659 - mse: 11.5659 - val_loss: 8.8906 - val_mse: 8.8906\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.4796 - mse: 11.4796 - val_loss: 8.7162 - val_mse: 8.7162\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.3700 - mse: 11.3700 - val_loss: 8.7195 - val_mse: 8.7195\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.3869 - mse: 11.3869 - val_loss: 8.4388 - val_mse: 8.4388\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.2810 - mse: 11.2810 - val_loss: 9.2150 - val_mse: 9.2150\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.3390 - mse: 11.3390 - val_loss: 8.6969 - val_mse: 8.6969\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.1230 - mse: 11.1230 - val_loss: 8.6291 - val_mse: 8.6291\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.9832 - mse: 10.9832 - val_loss: 8.4056 - val_mse: 8.4056\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.9473 - mse: 10.9473 - val_loss: 8.3451 - val_mse: 8.3451\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.9019 - mse: 10.9019 - val_loss: 8.4109 - val_mse: 8.4109\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.9031 - mse: 10.9031 - val_loss: 8.3521 - val_mse: 8.3521\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.8028 - mse: 10.8028 - val_loss: 8.4532 - val_mse: 8.4532\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.7302 - mse: 10.7302 - val_loss: 8.5037 - val_mse: 8.5037\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.6407 - mse: 10.6407 - val_loss: 8.3529 - val_mse: 8.3529\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.5706 - mse: 10.5706 - val_loss: 8.4330 - val_mse: 8.4330\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.5146 - mse: 10.5146 - val_loss: 8.3409 - val_mse: 8.3409\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.4604 - mse: 10.4604 - val_loss: 8.3863 - val_mse: 8.3863\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.3828 - mse: 10.3828 - val_loss: 8.2275 - val_mse: 8.2275\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.3175 - mse: 10.3175 - val_loss: 8.2748 - val_mse: 8.2748\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.3163 - mse: 10.3163 - val_loss: 8.2682 - val_mse: 8.2682\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.3026 - mse: 10.3026 - val_loss: 8.1345 - val_mse: 8.1345\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.2405 - mse: 10.2405 - val_loss: 8.1184 - val_mse: 8.1184\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.1940 - mse: 10.1940 - val_loss: 8.1114 - val_mse: 8.1114\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.0664 - mse: 10.0664 - val_loss: 8.0701 - val_mse: 8.0701\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.1385 - mse: 10.1385 - val_loss: 8.1966 - val_mse: 8.1966\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.2260 - mse: 10.2260 - val_loss: 7.9749 - val_mse: 7.9749\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.0789 - mse: 10.0789 - val_loss: 8.2251 - val_mse: 8.2251\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.9279 - mse: 9.9279 - val_loss: 8.0041 - val_mse: 8.0041\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.7875 - mse: 9.7875 - val_loss: 8.1352 - val_mse: 8.1352\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.7783 - mse: 9.7783 - val_loss: 7.9924 - val_mse: 7.9924\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.7867 - mse: 9.7867 - val_loss: 8.1221 - val_mse: 8.1221\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.7546 - mse: 9.7546 - val_loss: 7.9992 - val_mse: 7.9992\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.6291 - mse: 9.6291 - val_loss: 8.1437 - val_mse: 8.1437\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.6593 - mse: 9.6593 - val_loss: 8.1186 - val_mse: 8.1186\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.5162 - mse: 9.5162 - val_loss: 8.1026 - val_mse: 8.1026\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.5751 - mse: 9.5751 - val_loss: 8.1908 - val_mse: 8.1908\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.4800 - mse: 9.4800 - val_loss: 8.0492 - val_mse: 8.0492\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.4060 - mse: 9.4060 - val_loss: 8.0682 - val_mse: 8.0682\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.4166 - mse: 9.4166 - val_loss: 8.1175 - val_mse: 8.1175\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.3003 - mse: 9.3003 - val_loss: 8.0207 - val_mse: 8.0207\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.2602 - mse: 9.2602 - val_loss: 8.0574 - val_mse: 8.0574\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.2214 - mse: 9.2214 - val_loss: 8.0015 - val_mse: 8.0015\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.2796 - mse: 9.2796 - val_loss: 8.0907 - val_mse: 8.0907\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.2266 - mse: 9.2266 - val_loss: 7.9730 - val_mse: 7.9730\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.1083 - mse: 9.1083 - val_loss: 8.0281 - val_mse: 8.0281\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.1864 - mse: 9.1864 - val_loss: 8.0355 - val_mse: 8.0355\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.0886 - mse: 9.0886 - val_loss: 7.9930 - val_mse: 7.9930\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.0253 - mse: 9.0253 - val_loss: 8.1576 - val_mse: 8.1576\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.9331 - mse: 8.9331 - val_loss: 8.0063 - val_mse: 8.0063\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9126 - mse: 8.9126 - val_loss: 8.0679 - val_mse: 8.0679\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.8811 - mse: 8.8811 - val_loss: 7.9712 - val_mse: 7.9712\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9316 - mse: 8.9316 - val_loss: 8.0097 - val_mse: 8.0097\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.8200 - mse: 8.8200 - val_loss: 7.9540 - val_mse: 7.9540\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.7767 - mse: 8.7767 - val_loss: 7.9299 - val_mse: 7.9299\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.7015 - mse: 8.7015 - val_loss: 7.9764 - val_mse: 7.9764\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.7189 - mse: 8.7189 - val_loss: 7.8686 - val_mse: 7.8686\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.6659 - mse: 8.6659 - val_loss: 7.8908 - val_mse: 7.8908\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.5773 - mse: 8.5773 - val_loss: 7.8909 - val_mse: 7.8909\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.6295 - mse: 8.6295 - val_loss: 7.7965 - val_mse: 7.7965\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.5371 - mse: 8.5371 - val_loss: 7.7676 - val_mse: 7.7676\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.5474 - mse: 8.5474 - val_loss: 7.7396 - val_mse: 7.7396\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.6240 - mse: 8.6240 - val_loss: 7.7307 - val_mse: 7.7307\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.4918 - mse: 8.4918 - val_loss: 7.7875 - val_mse: 7.7875\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.4904 - mse: 8.4904 - val_loss: 7.7855 - val_mse: 7.7855\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.4380 - mse: 8.4380 - val_loss: 7.7405 - val_mse: 7.7405\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.4077 - mse: 8.4077 - val_loss: 7.7474 - val_mse: 7.7474\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.2620 - mse: 8.2620 - val_loss: 7.7924 - val_mse: 7.7924\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.3204 - mse: 8.3204 - val_loss: 7.7646 - val_mse: 7.7646\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.2628 - mse: 8.2628 - val_loss: 7.7187 - val_mse: 7.7187\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.2416 - mse: 8.2416 - val_loss: 7.7545 - val_mse: 7.7545\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.2251 - mse: 8.2251 - val_loss: 7.7686 - val_mse: 7.7686\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.2529 - mse: 8.2529 - val_loss: 7.7875 - val_mse: 7.7875\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.1665 - mse: 8.1665 - val_loss: 7.7304 - val_mse: 7.7304\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.1639 - mse: 8.1639 - val_loss: 7.6539 - val_mse: 7.6539\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.1578 - mse: 8.1578 - val_loss: 7.7388 - val_mse: 7.7388\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.1309 - mse: 8.1309 - val_loss: 7.7677 - val_mse: 7.7677\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.9894 - mse: 7.9894 - val_loss: 7.7088 - val_mse: 7.7088\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.9977 - mse: 7.9977 - val_loss: 7.5983 - val_mse: 7.5983\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.9362 - mse: 7.9362 - val_loss: 7.5656 - val_mse: 7.5656\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8875 - mse: 7.8875 - val_loss: 7.5975 - val_mse: 7.5975\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.0171 - mse: 8.0171 - val_loss: 7.5808 - val_mse: 7.5808\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8895 - mse: 7.8895 - val_loss: 7.4656 - val_mse: 7.4656\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.0083 - mse: 8.0083 - val_loss: 7.5198 - val_mse: 7.5198\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8136 - mse: 7.8136 - val_loss: 7.6175 - val_mse: 7.6175\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.8479 - mse: 7.8479 - val_loss: 7.5153 - val_mse: 7.5153\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7930 - mse: 7.7930 - val_loss: 7.4537 - val_mse: 7.4537\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6159 - mse: 7.6159 - val_loss: 7.6167 - val_mse: 7.6167\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6780 - mse: 7.6780 - val_loss: 7.4517 - val_mse: 7.4517\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5675 - mse: 7.5675 - val_loss: 7.4466 - val_mse: 7.4466\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.5846 - mse: 7.5846 - val_loss: 7.4231 - val_mse: 7.4231\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6293 - mse: 7.6293 - val_loss: 7.4711 - val_mse: 7.4711\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7291 - mse: 7.7291 - val_loss: 7.5377 - val_mse: 7.5377\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.5972 - mse: 7.5972 - val_loss: 7.4670 - val_mse: 7.4670\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5024 - mse: 7.5024 - val_loss: 7.3784 - val_mse: 7.3784\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.4429 - mse: 7.4429 - val_loss: 7.3531 - val_mse: 7.3531\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4288 - mse: 7.4288 - val_loss: 7.3196 - val_mse: 7.3196\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.3967 - mse: 7.3967 - val_loss: 7.4806 - val_mse: 7.4806\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.5037 - mse: 7.5037 - val_loss: 7.3224 - val_mse: 7.3224\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.5227 - mse: 7.5227 - val_loss: 7.3062 - val_mse: 7.3062\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.4313 - mse: 7.4313 - val_loss: 7.2915 - val_mse: 7.2915\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3307 - mse: 7.3307 - val_loss: 7.2636 - val_mse: 7.2636\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2069 - mse: 7.2069 - val_loss: 7.3631 - val_mse: 7.3631\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2094 - mse: 7.2094 - val_loss: 7.2893 - val_mse: 7.2893\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2290 - mse: 7.2290 - val_loss: 7.2225 - val_mse: 7.2225\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1545 - mse: 7.1545 - val_loss: 7.3049 - val_mse: 7.3049\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0945 - mse: 7.0945 - val_loss: 7.2706 - val_mse: 7.2706\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1604 - mse: 7.1604 - val_loss: 7.1775 - val_mse: 7.1775\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.1362 - mse: 7.1362 - val_loss: 7.2049 - val_mse: 7.2049\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1129 - mse: 7.1129 - val_loss: 7.4328 - val_mse: 7.4328\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.0929 - mse: 7.0929 - val_loss: 7.2102 - val_mse: 7.2102\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9602 - mse: 6.9602 - val_loss: 7.3628 - val_mse: 7.3628\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9542 - mse: 6.9542 - val_loss: 7.2760 - val_mse: 7.2760\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8613 - mse: 6.8613 - val_loss: 7.2213 - val_mse: 7.2213\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9219 - mse: 6.9219 - val_loss: 7.2239 - val_mse: 7.2239\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8905 - mse: 6.8905 - val_loss: 7.3119 - val_mse: 7.3119\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9212 - mse: 6.9212 - val_loss: 7.2304 - val_mse: 7.2304\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9755 - mse: 6.9755 - val_loss: 7.1675 - val_mse: 7.1675\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7821 - mse: 6.7821 - val_loss: 7.2966 - val_mse: 7.2966\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7432 - mse: 6.7432 - val_loss: 7.1419 - val_mse: 7.1419\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7252 - mse: 6.7252 - val_loss: 7.2018 - val_mse: 7.2018\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7008 - mse: 6.7008 - val_loss: 7.1272 - val_mse: 7.1272\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6599 - mse: 6.6599 - val_loss: 7.1622 - val_mse: 7.1622\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6631 - mse: 6.6631 - val_loss: 7.0361 - val_mse: 7.0361\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6779 - mse: 6.6779 - val_loss: 7.3019 - val_mse: 7.3019\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.5151 - mse: 6.5151 - val_loss: 7.0326 - val_mse: 7.0326\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6446 - mse: 6.6446 - val_loss: 7.0443 - val_mse: 7.0443\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6853 - mse: 6.6853 - val_loss: 7.1437 - val_mse: 7.1437\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5309 - mse: 6.5309 - val_loss: 7.1457 - val_mse: 7.1457\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5265 - mse: 6.5265 - val_loss: 7.2324 - val_mse: 7.2324\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.4252 - mse: 6.4252 - val_loss: 7.0880 - val_mse: 7.0880\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5497 - mse: 6.5497 - val_loss: 7.2144 - val_mse: 7.2144\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5180 - mse: 6.5180 - val_loss: 7.3195 - val_mse: 7.3195\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3806 - mse: 6.3806 - val_loss: 7.1878 - val_mse: 7.1878\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3399 - mse: 6.3399 - val_loss: 7.1310 - val_mse: 7.1310\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2808 - mse: 6.2808 - val_loss: 7.1707 - val_mse: 7.1707\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2710 - mse: 6.2710 - val_loss: 7.1932 - val_mse: 7.1932\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2885 - mse: 6.2885 - val_loss: 7.1495 - val_mse: 7.1495\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2464 - mse: 6.2464 - val_loss: 7.1105 - val_mse: 7.1105\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2295 - mse: 6.2295 - val_loss: 7.0420 - val_mse: 7.0420\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1593 - mse: 6.1593 - val_loss: 7.3184 - val_mse: 7.3184\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0994 - mse: 6.0994 - val_loss: 7.2664 - val_mse: 7.2664\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1806 - mse: 6.1806 - val_loss: 7.2139 - val_mse: 7.2139\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0633 - mse: 6.0633 - val_loss: 7.2125 - val_mse: 7.2125\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1704 - mse: 6.1704 - val_loss: 7.3311 - val_mse: 7.3311\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0404 - mse: 6.0404 - val_loss: 7.2323 - val_mse: 7.2323\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1017 - mse: 6.1017 - val_loss: 7.1639 - val_mse: 7.1639\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1158 - mse: 6.1158 - val_loss: 7.3312 - val_mse: 7.3312\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1068 - mse: 6.1068 - val_loss: 7.2473 - val_mse: 7.2473\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0070 - mse: 6.0070 - val_loss: 7.1831 - val_mse: 7.1831\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8332 - mse: 5.8332 - val_loss: 7.2605 - val_mse: 7.2605\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0127 - mse: 6.0127 - val_loss: 7.0858 - val_mse: 7.0858\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8427 - mse: 5.8427 - val_loss: 7.1113 - val_mse: 7.1113\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8994 - mse: 5.8994 - val_loss: 7.3581 - val_mse: 7.3581\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7198 - mse: 5.7198 - val_loss: 7.0762 - val_mse: 7.0762\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7404 - mse: 5.7404 - val_loss: 7.0684 - val_mse: 7.0684\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8789 - mse: 5.8789 - val_loss: 7.3595 - val_mse: 7.3595\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8742 - mse: 5.8742 - val_loss: 6.9621 - val_mse: 6.9621\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8785 - mse: 5.8785 - val_loss: 7.4233 - val_mse: 7.4233\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.7035 - mse: 5.7035 - val_loss: 7.0805 - val_mse: 7.0805\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5742 - mse: 5.5742 - val_loss: 7.3451 - val_mse: 7.3451\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.6027 - mse: 5.6027 - val_loss: 7.2334 - val_mse: 7.2334\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.6208 - mse: 5.6208 - val_loss: 7.1633 - val_mse: 7.1633\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 5.5406 - mse: 5.5406 - val_loss: 7.2663 - val_mse: 7.2663\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5413 - mse: 5.5413 - val_loss: 7.2242 - val_mse: 7.2242\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.6795 - mse: 5.6795 - val_loss: 7.3426 - val_mse: 7.3426\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.6466 - mse: 5.6466 - val_loss: 7.1124 - val_mse: 7.1124\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.6070 - mse: 5.6070 - val_loss: 8.0179 - val_mse: 8.0179\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9304 - mse: 5.9304 - val_loss: 7.3117 - val_mse: 7.3117\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.4974 - mse: 5.4974 - val_loss: 7.3878 - val_mse: 7.3878\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.4097 - mse: 5.4097 - val_loss: 7.2147 - val_mse: 7.2147\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5180 - mse: 5.5180 - val_loss: 7.3816 - val_mse: 7.3816\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.5341 - mse: 5.5341 - val_loss: 7.2781 - val_mse: 7.2781\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f191ec63650>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_features)\n",
        "train_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn10UwAo3ylh",
        "outputId": "2e17bf47-331c-4305-ad34-933d755d9f86"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4, 12.1,\n",
              "       17.9, 23.1, 19.9, 15.7,  8.8, 50. , 22.5, 24.1, 27.5, 10.9, 30.8,\n",
              "       32.9, 24. , 18.5, 13.3, 22.9, 34.7, 16.6, 17.5, 22.3, 16.1, 14.9,\n",
              "       23.1, 34.9, 25. , 13.9, 13.1, 20.4, 20. , 15.2, 24.7, 22.2, 16.7,\n",
              "       12.7, 15.6, 18.4, 21. , 30.1, 15.1, 18.7,  9.6, 31.5, 24.8, 19.1,\n",
              "       22. , 14.5, 11. , 32. , 29.4, 20.3, 24.4, 14.6, 19.5, 14.1, 14.3,\n",
              "       15.6, 10.5,  6.3, 19.3, 19.3, 13.4, 36.4, 17.8, 13.5, 16.5,  8.3,\n",
              "       14.3, 16. , 13.4, 28.6, 43.5, 20.2, 22. , 23. , 20.7, 12.5, 48.5,\n",
              "       14.6, 13.4, 23.7, 50. , 21.7, 39.8, 38.7, 22.2, 34.9, 22.5, 31.1,\n",
              "       28.7, 46. , 41.7, 21. , 26.6, 15. , 24.4, 13.3, 21.2, 11.7, 21.7,\n",
              "       19.4, 50. , 22.8, 19.7, 24.7, 36.2, 14.2, 18.9, 18.3, 20.6, 24.6,\n",
              "       18.2,  8.7, 44. , 10.4, 13.2, 21.2, 37. , 30.7, 22.9, 20. , 19.3,\n",
              "       31.7, 32. , 23.1, 18.8, 10.9, 50. , 19.6,  5. , 14.4, 19.8, 13.8,\n",
              "       19.6, 23.9, 24.5, 25. , 19.9, 17.2, 24.6, 13.5, 26.6, 21.4, 11.9,\n",
              "       22.6, 19.6,  8.5, 23.7, 23.1, 22.4, 20.5, 23.6, 18.4, 35.2, 23.1,\n",
              "       27.9, 20.6, 23.7, 28. , 13.6, 27.1, 23.6, 20.6, 18.2, 21.7, 17.1,\n",
              "        8.4, 25.3, 13.8, 22.2, 18.4, 20.7, 31.6, 30.5, 20.3,  8.8, 19.2,\n",
              "       19.4, 23.1, 23. , 14.8, 48.8, 22.6, 33.4, 21.1, 13.6, 32.2, 13.1,\n",
              "       23.4, 18.9, 23.9, 11.8, 23.3, 22.8, 19.6, 16.7, 13.4, 22.2, 20.4,\n",
              "       21.8, 26.4, 14.9, 24.1, 23.8, 12.3, 29.1, 21. , 19.5, 23.3, 23.8,\n",
              "       17.8, 11.5, 21.7, 19.9, 25. , 33.4, 28.5, 21.4, 24.3, 27.5, 33.1,\n",
              "       16.2, 23.3, 48.3, 22.9, 22.8, 13.1, 12.7, 22.6, 15. , 15.3, 10.5,\n",
              "       24. , 18.5, 21.7, 19.5, 33.2, 23.2,  5. , 19.1, 12.7, 22.3, 10.2,\n",
              "       13.9, 16.3, 17. , 20.1, 29.9, 17.2, 37.3, 45.4, 17.8, 23.2, 29. ,\n",
              "       22. , 18. , 17.4, 34.6, 20.1, 25. , 15.6, 24.8, 28.2, 21.2, 21.4,\n",
              "       23.8, 31. , 26.2, 17.4, 37.9, 17.5, 20. ,  8.3, 23.9,  8.4, 13.8,\n",
              "        7.2, 11.7, 17.1, 21.6, 50. , 16.1, 20.4, 20.6, 21.4, 20.6, 36.5,\n",
              "        8.5, 24.8, 10.8, 21.9, 17.3, 18.9, 36.2, 14.9, 18.2, 33.3, 21.8,\n",
              "       19.7, 31.6, 24.8, 19.4, 22.8,  7.5, 44.8, 16.8, 18.7, 50. , 50. ,\n",
              "       19.5, 20.1, 50. , 17.2, 20.8, 19.3, 41.3, 20.4, 20.5, 13.8, 16.5,\n",
              "       23.9, 20.6, 31.5, 23.3, 16.8, 14. , 33.8, 36.1, 12.8, 18.3, 18.7,\n",
              "       19.1, 29. , 30.1, 50. , 50. , 22. , 11.9, 37.6, 50. , 22.7, 20.8,\n",
              "       23.5, 27.9, 50. , 19.3, 23.9, 22.6, 15.2, 21.7, 19.2, 43.8, 20.3,\n",
              "       33.2, 19.9, 22.5, 32.7, 22. , 17.1, 19. , 15. , 16.1, 25.1, 23.7,\n",
              "       28.7, 37.2, 22.6, 16.4, 25. , 29.8, 22.1, 17.4, 18.1, 30.3, 17.5,\n",
              "       24.7, 12.6, 26.5, 28.7, 13.3, 10.4, 24.4, 23. , 20. , 17.8,  7. ,\n",
              "       11.8, 24.4, 13.8, 19.4, 25.2, 19.4, 19.4, 29.1])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEFaa0VQaKef"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "Now that you know how MSE works, you need to plot the behavior of MSE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for i in range(len(train_features)):\n",
        "  #print(i)\n",
        "\n",
        "a = (1, 2)\n",
        "x = sum(a)\n",
        "print(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFirsiAl4bMz",
        "outputId": "072dce1c-1d5b-49e6-8b33-9c0b01b24cc6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc5OFsCmadXE"
      },
      "source": [
        "### Answer 1\n",
        "# is mse=errors/n correct? Yes, but dont take the mean of \"errors\" here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8RF_LUDbdo2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "f652d076-3318-4bdc-d632-9b6aa3daf67b"
      },
      "source": [
        "import numpy as np\n",
        "errors = np.arange(-5, 6)\n",
        "n = len(errors)\n",
        "#for i in range(len(train_features)):\n",
        "\n",
        "  #mse =sum(( train_labels-train_labels[i])**2)/(len(train_features))\n",
        "mse=errors**2/n\n",
        "plt.plot(errors, mse, c='#ED4F46', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Difference between actual and estimated ouputs')\n",
        "plt.ylabel('Mean Squared Errors')\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OR FOR MSE USE THIS\n",
        "'''\n",
        "import numpy as np\n",
        "\n",
        "errors = np.random.randint(5, size=(2, 4))\n",
        "n = len(errors)\n",
        "#for i in range(len(train_features)):\n",
        "\n",
        "  #mse =sum(( train_labels-train_labels[i])**2)/(len(train_features))\n",
        "mse=np.sum(errors**2, axis=0)/n\n",
        "plt.plot(errors, mse, c='#ED4F46', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Difference between actual and estimated ouputs')\n",
        "plt.ylabel('Mean Squared Errors')\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "8D8EMH8kdC0S",
        "outputId": "d418d06e-6cbb-4370-cc19-4126a7e58208"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-ea2f00313dc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m#mse =sum(( train_labels-train_labels[i])**2)/(len(train_features))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'#ED4F46'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Difference between actual and estimated ouputs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (4,) and (1,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aShHwxvw6hml"
      },
      "source": [
        "## Mean Absolute Error [MAE]\n",
        "\n",
        "Mean absolute error, on the other hand, is measured as the average of sum of absolute differences between predictions and actual observations. \n",
        "\n",
        "Like MSE, this as well measures the magnitude of error without considering their direction. \n",
        "\n",
        "Unlike MSE, MAE needs more complicated tools such as linear programming to compute the gradients. Plus MAE is more robust to outliers since it does not make use of square.\n",
        "\n",
        "Let's assume there are $n$ data samples, for $i^{th}$ sample; the actual output is $y_i$ and $\\hat{y}_i$ is the estimated output from the regression model. \n",
        "\n",
        "We first take the absolute difference between the original and estimated output with $|y_i - \\hat{y}_i|2$. Then we take sum of the absolute differences for all the samples. And finally divide it by the total count of samples, which is $n$. \n",
        "\n",
        "$$MSE = \\frac{\\sum_{i=1}^{n}|y_i - \\hat{y}_i|}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI9_GEL86hmn"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mae',\n",
        "              metrics=['mae'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IEl3-k3bn7N"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "Now that you know how MAE works, you need to plot the behavior of MAE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYyGYH4zbn7N"
      },
      "source": [
        "### Answer 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK1qdGtBbn7N"
      },
      "source": [
        "errors = np.arange(-5, 6)\n",
        "n = len(errors)\n",
        "\n",
        "mae = abs(errors)/n\n",
        "\n",
        "plt.plot(errors, mae, c='#0095B6', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Difference between actual and estimated ouputs')\n",
        "plt.ylabel('Mean Absolute Errors')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIFJ88ER6s7w"
      },
      "source": [
        "## Mean Squared Logarithmic Error [MSLE]\n",
        "\n",
        "MSLE is just like MSE, but we have to take $log$ of the actual and estimated outputs because squaring and averaging. \n",
        "\n",
        "The introduction of the logarithm makes MSLE only care about the relative difference between the true and the predicted value, or in other words, it only cares about the percentual difference between them.\n",
        "\n",
        "This means that MSLE will treat small differences between small true and predicted values approximately the same as big differences between large true and predicted values.\n",
        "\n",
        "We can use MSLE when we don't want large errors to be significantly more penalized than small ones, in those cases where the range of the target value is large.\n",
        "\n",
        "*Example*: You want to predict future house prices, and your dataset includes homes that are orders of magnitude different in price. The price is a continuous value, and therefore, we want to do regression. MSLE can here be used as the loss function.\n",
        "\n",
        "$$MSLE = \\frac{\\sum_{i=1}^{n}(\\log(y_i+1) - \\log(\\hat{y}_i+1))^2}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBkzaP9R7KnB"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss=tf.keras.losses.MeanSquaredLogarithmicError(),\n",
        "              metrics=['mean_squared_logarithmic_error'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=150, validation_split = 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc_jN5XwcBu9"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "Now that you know how MSLE works, you need to plot the behavior of MSLE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_USeo3P7cBu-"
      },
      "source": [
        "### Answer 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHr_XxgUcBu-"
      },
      "source": [
        "import numpy as np\n",
        "actual_outputs = np.arange(0, 51)\n",
        "n = len(actual_outputs)\n",
        "estimated_outputs = np.zeros(n)\n",
        "\n",
        "msle = ((np.log(actual_outputs + 1) - np.log(estimated_outputs + 1))**2)/n\n",
        "\n",
        "plt.plot(actual_outputs, msle, c='#F47789', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Actual ouputs')\n",
        "plt.ylabel('Mean Squared Logarithmic Errors')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BylzTZ1W9Ma"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "Why do we add $1$ to the outputs before passing it through $\\log()$? \n",
        "\n",
        "#myanswer\n",
        "\n",
        "bc anything that gets passed into log(a) comes out as a value x that is 10^x=a. adding 1 would make this value bigger. also,\n",
        "The reason ‘1’ is added to both y and ŷ is for mathematical convenience since log(0)is not defined but both y or ŷ can be 0. good to +1 in case output of the logx is 0. its also to ensure that all output values are above 0 because log0 is undefined.\n",
        "\n",
        "if just in case y=0 log0 cant work(undef) so add+1 to avoid 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaQiO_XeYTjE"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "Write your observations about MSE, MAE, and MSLE; and compare the results achieved with all 3 loss functions. \n",
        "\n",
        "#myanswer\n",
        "##mse includes squared - which is why graph of it is quadratic function like x^2. penalizes large errors harshly (steep curve)\n",
        "##mae includes absolute error - so absolute value function is what we see in the graph like v shape. penalizes error linearly (bc 2 straight lines). not penalize large errors as harshly as mse.\n",
        "##msle includes log - so function graph is a log function. penalizes large errors not harshly at all, in fact smaller and larger errors get similar penalty\n",
        "\n",
        "\n",
        "\n",
        "as for results, in the last epoch:\n",
        "val_loss: 1.6863 - val_mae: 1.6863\n",
        "val_loss: 0.0181 - val_mean_squared_logarithmic_error: 0.0181\n",
        "val_loss: 8.3187 - val_mse: 8.3187\n",
        "\n",
        "so it looks like msle achieved the lowest loss out of all of them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efp4KP5GfDL7"
      },
      "source": [
        "## Question 6\n",
        "\n",
        "Plug-in any of the loss functions from [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/losses) docs to the `model.compile` method and see if the difference in model performance as compared to MSE, MAE, and MSLE.\n",
        "\n",
        "#myanswer\n",
        "validation mse hovered around 5 or 6, which is still pretty big loss. <br>\n",
        "validation mae was significantly smaller at around 1.8, but never decreased(1.8 for pretty much all 250 epochs)<br>\n",
        "validation msle was significantly smaller at around 0.015, but never decreased(0.015 for pretty much all 250 epochs). msle achieved the lowest loss out of all of them.\n",
        "\n",
        "#why did all 3 losses not decrease? why is my model not learning?\n",
        "the 3 losses decreased more for train, less for validation testing. 3 losses decreased slower there. means my model is learning training data more than testing data. its still learning tho, just that the losses are stuck in local min. losses  not close to 0 bc its stuck in local min not global min\n",
        "\n",
        "#what should we put for loss? anything like hinge or cross entropy or pick one of mse mae msle?? \n",
        "can only pick mse mae msle bc this is a regression task. loss=cross entropy is used for classification tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDWlkms1flkZ"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss= 'mae',\n",
        "              metrics=[\"mse\", \"mae\", \"msle\"])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mupD9JvzD1BU"
      },
      "source": [
        "#Fun Fact\n",
        "\n",
        "Google Translate is getting better all the time, but it's still not perfect. Translate a sentence into another language and back into English, and you might get a hilarious surprise. That's what Malinda Kathleen Reese got when she reverse Google Translated the lyrics to \"Let It Go\" from Disney's Frozen into Chinese, Macedonian, French, Polish, Creole, Tamil and others. It doesn't come out as utter gibberish, but as a slightly off version with a slightly different message from the original. Which makes it even funnier. Plus, Malinda can really sing.\n",
        "\n",
        "Link to video: https://www.youtube.com/watch?v=2bVAoVlFYf0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t0A5Xui7sum"
      },
      "source": [
        "# Classification Losses\n",
        "\n",
        "In classification, the outputs are in form of a class or a category. The label or number assigned to the classes do not have a numerical meaning. \n",
        "\n",
        "For example, an input with class label 0 cannot be numerically compared with an input with class label 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERGOjkbwjCT1"
      },
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = tf.keras.datasets.cifar100.load_data(label_mode=\"coarse\")\n",
        "\n",
        "training_images=training_images.reshape(50000, 32, 32, 3)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 32, 32, 3)\n",
        "test_images=test_images/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EbtSlPYZYju"
      },
      "source": [
        "## Kullback-Leibler Divergence [KDL]\n",
        "\n",
        "Kullback Leibler Divergence Loss is a measure of how a distribution varies from a reference distribution (or a baseline distribution). A Kullback Leibler Divergence Loss of zero means that both the probability distributions are identical.\n",
        "\n",
        "The number of information lost in the predicted distribution is used as a measure.\n",
        "\n",
        "$$KDL(p||q) = \\int_x p(x) \\log \\frac{p(x)}{q(x)} dx$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkyXvdIQZZyO"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='kl_divergence', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIOrqUGTYjD8"
      },
      "source": [
        "##Binary Cross Entropy\n",
        "Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.\n",
        "\n",
        "Cross-entropy is also related to and often confused with logistic loss, called log loss. Although the two measures are derived from a different source, when used as loss functions for classification models, both measures calculate the same quantity and can be used interchangeably.\n",
        "\n",
        "Binary crossentropy is a loss function that is used in binary classification tasks. These are tasks that answer a question with only two choices (yes or no, A or B, 0 or 1, left or right). Several independent such questions can be answered at the same time, as in multi-label classification or in binary image segmentation. Formally, this loss is equal to the average of the categorical crossentropy loss on many two-category tasks.\n",
        "\n",
        "$$BCE = -\\frac{1}{N} \\sum_{i=1}^N y_i \\cdot \\log(p(y_i)) + (1-y_i) \\cdot \\log(1- p(y_i))$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels = tf.reshape(tf.one_hot(training_labels, 100), [training_labels.shape[0], 100])\n",
        "print(training_labels.shape)\n",
        "\n",
        "test_labels = tf.reshape(tf.one_hot(test_labels, 100), [test_labels.shape[0], 100])\n",
        "print(test_labels.shape)"
      ],
      "metadata": {
        "id": "Ckc0jYu_LwkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRvhNnnJYjOF"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7\n",
        "\n",
        "Do you see any problems/errors with the above code? Please describe.\n",
        "#myanswer\n",
        "yes they are using binary cross entropy as the loss for this multiclass task. binary crossentropy should only be used for 2 classes/binary classification tasks. (yes or no, etc). however, this task involves 20 classes\n",
        "\n"
      ],
      "metadata": {
        "id": "eAAuecZfTfx2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDBdn-P976v9"
      },
      "source": [
        "## Categorical Cross Entropy\n",
        "\n",
        "This is the most common setting for classification problems. Cross-entropy loss increases as the **predicted probability** strays away from the **actual label**.\n",
        "\n",
        "Note that we have to compare the probabilities (e.g. [0.20, 0.75, 0.05]) of all the classes with the actual labels (e.g., [0, 1, 0]). The actual labels would be one-hot encoding.\n",
        "\n",
        "An important aspect of this is that cross entropy loss penalizes heavily the predictions that are confident but wrong.\n",
        "\n",
        "We are multiplying the log of the actual predicted probability for the ground truth class.\n",
        "\n",
        "$$CCE = -\\frac{1}{N}\\sum_{i=1}^{N}y_i\\log(\\hat{y}_i)$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uskCvsUSrR0o"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=25, validation_data=(test_images, test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8\n",
        "\n",
        "Now that you know how CCE works, you need to code it. It should give the same answer as `tf.keras.metrics.categorical_crossentropy` would."
      ],
      "metadata": {
        "id": "KYuLKOGSR4yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 8"
      ],
      "metadata": {
        "id": "TAtqr73-SLUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#what does N stand for in the categorical cross entropy equation? length of training data so len(true)=len(pred) must be equal\n",
        "\n",
        "#why must use np.sum and np.log and not sum() and math.log\n",
        "tf.math.log works too"
      ],
      "metadata": {
        "id": "C-pU6ig4GrAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math\n",
        "def categorical_crossentropy(true, pred):\n",
        "    \n",
        "    loss=np.sum(true*tf.math.log(pred),axis=None) /(-len(pred))\n",
        "    \n",
        "\n",
        "    return loss\n",
        "\n",
        "true = tf.constant([[0.0, 1.0, 0.0],\n",
        "                    [1.0, 0.0, 0.0],\n",
        "                    [1.0, 0.0, 0.0],\n",
        "                    [0.0, 0.0, 1.0]])\n",
        "pred = tf.constant([[0.20, 0.70, 0.10],\n",
        "                    [0.80, 0.05, 0.15],\n",
        "                    [0.75, 0.10, 0.15],\n",
        "                    [0.25, 0.15, 0.60]])\n",
        "\n",
        "loss = categorical_crossentropy(true, pred)\n",
        "print(loss)\n",
        "\n",
        "loss = tf.keras.metrics.categorical_crossentropy(true, pred)\n",
        "loss = tf.reduce_mean(loss)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "mxzrT-GKSRSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CHuqni18OPL"
      },
      "source": [
        "## Sparse Categorical Cross Entropy\n",
        "\n",
        "Both, Categorical Cross Entropy [CCE] and Sparse Categorical Cross Entropy [SCCE] have the same loss function. The only difference is the format of $y_i$ (i.e., true labels).\n",
        "\n",
        "If $y_i$'s are one-hot encoded, we should use CCE. Examples (for a 3-class classification): [1,0,0], [0,1,0], [0,0,1]\n",
        "\n",
        "But if $y_i$'s are integers, use SCCE. Examples for above 3-class classification problem: [1], [2], [3]\n",
        "\n",
        "The usage entirely depends on how we load our dataset. One advantage of using sparse categorical cross entropy is it saves time in memory as well as computation because it simply uses a single integer for a class, rather than a whole vector.\n",
        "\n",
        "$$SCCE = -\\log(\\hat{y}_i)$$ for $i$ where $one\\text{-}hot\\text{-}encoding[i] = 1$ "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = tf.keras.datasets.cifar100.load_data(label_mode=\"coarse\")\n",
        "\n",
        "training_images=training_images.reshape(50000, 32, 32, 3)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 32, 32, 3)\n",
        "test_images=test_images/255.0"
      ],
      "metadata": {
        "id": "tb6GisE4SkhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcFK26KvCp-g"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhV-fVzcNc3p"
      },
      "source": [
        "## Question 9\n",
        "\n",
        "What is the difference between a Multi-class and Multi-label Classification problem, and what sort of loss function would we need to learn them?\n",
        "#myanswer\n",
        "Multi-class classification - 1 class for each input only. 1:1 (one input one output) - use sparse categorical cross entropy loss \n",
        "\n",
        "Multi-label classification - intput can have multiple classes (this image is both a cat and a lion) - use categorical cross entropy loss \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITEN1-3-OQwd"
      },
      "source": [
        "## Question 10\n",
        "What is the relationship between Binary Cross entropy and Categorical Cross entropy?\n",
        "\n",
        "#myanswer\n",
        "both use cross entropy\n",
        "\n",
        "differences: binary one is for 2-class classification (binary). categorical one is used for multiple classes (3 or more)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGwk3Z0xOmRk"
      },
      "source": [
        "## Question 11\n",
        "\n",
        "What is the relationship between Sparse Cross entropy and Categorical Cross entropy?\n",
        "\n",
        "#myanswer\n",
        "both use categorical loss functions but sparse's input are integers and categorical's are arrays\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RDBtUuYjuFh"
      },
      "source": [
        "# **Upload this Day 9 Colab Notebook to your Github repository under \"Day 9\" folder. Also add your *Reflection* on today's learning in README.md**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgg0bvRjS9un"
      },
      "source": [
        "Sources:\n",
        "\n",
        "https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23\n",
        "\n",
        "https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-logarithmic-error-(msle)"
      ]
    }
  ]
}